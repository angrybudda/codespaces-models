{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with GitHub Models - OpenAI SDK and embeddings\n",
    "\n",
    "## 1. Personal access token\n",
    "\n",
    "A personal access token is made available in the Codespaces environment in the `GITHUB_TOKEN` environment variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai --quiet\n",
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Set environment variables and create the client\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.getenv(\"GITHUB_TOKEN\"):\n",
    "    raise ValueError(\"GITHUB_TOKEN is not set\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://models.inference.ai.azure.com/\"\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Embed a string\n",
    "\n",
    "This is just calling the `embeddings.create` endpoint with a simple prompt.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.019184619188308716, -0.025279032066464424, -0.0017195191467180848, 0.01884828321635723, -0.033795066177845, -0.01969585195183754, -0.02094702236354351, 0.051580529659986496, -0.03212684020400047, -0.030377890914678574]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"text-embedding-3-small\" \n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=model_name,\n",
    "    input=[\"Hello, world!\"]\n",
    ")\n",
    "embeddings = response.data[0].embedding\n",
    "print(len(embeddings))\n",
    "print(embeddings[:10])\n",
    "print(len(response.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the response from the `text-embedding-3-small` model contains a vector of length 1536, which is the embedding of the input string. Different models have different embedding sizes. Please consult the [model documentation](https://github.com/marketplace/models/) for more information about the embedding model you are using. \n",
    "\n",
    "## 5. Embed a list of strings\n",
    "\n",
    "To save on API calls, you can embed a list of strings in a single call. The response will contain a list of embeddings, one for each input string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.019184619188308716, -0.025279032066464424, -0.0017195191467180848, 0.01884828321635723, -0.033795066177845, -0.01969585195183754, -0.02094702236354351, 0.051580529659986496, -0.03212684020400047, -0.030377890914678574]\n",
      "[0.027574976906180382, -0.026575712487101555, -0.027027759701013565, 0.057814665138721466, -0.0034171321894973516, -0.002841662149876356, 0.031809959560632706, 0.010177046060562134, -0.03414158150553703, -0.04465766251087189]\n",
      "[-0.030613932758569717, -0.006268044002354145, -0.03854816406965256, -0.008153949864208698, 0.0046720425598323345, 0.018029019236564636, 0.02045811340212822, -0.052683304995298386, 0.009106057696044445, 0.01601494289934635]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"text-embedding-3-small\" \n",
    "inputs = [\"Hello, world!\", \"How are you?\", \"What's the weather like?\"]\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    model=model_name,\n",
    "    input=inputs\n",
    ")\n",
    "for data in response.data:\n",
    "    print(data.embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "A common use case for embeddings in generative AI is to use them to implement Retrieval Augmented Generation (RAG) systems.\n",
    "\n",
    "See the cookbook [rag_getting_started](../../../cookbooks/python/llamaindex/rag_getting_started.ipynb) for an example of how to do this using the LLamaIndex framework.\n",
    "\n",
    "To learn more about what you can do with the GitHub models using the OpenAI Python API, [check out theses cookbooks](../../../cookbooks/python/openai/README.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gh-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
