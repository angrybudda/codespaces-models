{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7f6058-d0d1-44af-b43a-cb4b06df03d8",
   "metadata": {},
   "source": [
    "# Prompting Capabilities \n",
    "\n",
    "When you first start using Mistral models, your first interaction will revolve around prompts. The art of crafting effective prompts is essential for generating desirable responses from Mistral models or other LLMs. This guide will walk you through example prompts showing four different prompting capabilities. \n",
    "\n",
    "- Classification \n",
    "- Summarization \n",
    "- Personalization\n",
    "- Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa37d97-10e4-425d-b852-15bd043662b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mistralai in /usr/local/python/3.11.9/lib/python3.11/site-packages (0.4.2)\n",
      "Requirement already satisfied: httpx<1,>=0.25 in /usr/local/python/3.11.9/lib/python3.11/site-packages (from mistralai) (0.27.2)\n",
      "Requirement already satisfied: orjson<3.11,>=3.9.10 in /usr/local/python/3.11.9/lib/python3.11/site-packages (from mistralai) (3.10.7)\n",
      "Requirement already satisfied: pydantic<3,>=2.5.2 in /usr/local/python/3.11.9/lib/python3.11/site-packages (from mistralai) (2.8.2)\n",
      "Requirement already satisfied: anyio in /usr/local/python/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.25->mistralai) (4.4.0)\n",
      "Requirement already satisfied: certifi in /usr/local/python/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.25->mistralai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/python/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.25->mistralai) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/python/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.25->mistralai) (3.8)\n",
      "Requirement already satisfied: sniffio in /usr/local/python/3.11.9/lib/python3.11/site-packages (from httpx<1,>=0.25->mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/python/3.11.9/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.25->mistralai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=2.5.2->mistralai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/python/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=2.5.2->mistralai) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/python/3.11.9/lib/python3.11/site-packages (from pydantic<3,>=2.5.2->mistralai) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae93e75a-fd20-4ca6-9eff-bef7d44ab3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "import os, dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "assert \"GITHUB_TOKEN\" in os.environ, \"Please set the GITHUB_TOKEN environment variable.\"\n",
    "github_token = os.environ[\"GITHUB_TOKEN\"]\n",
    "\n",
    "# We can use some defaults for the other two variables\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "model_name = \"mistral-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e030d9c2-1ecb-4bf0-864d-9f96b41bd016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(user_message, model=model_name):\n",
    "    client = MistralClient(api_key=github_token, endpoint=endpoint)\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=user_message)\n",
    "    ]\n",
    "    chat_response = client.chat(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98a29ef-4764-40b7-aed8-0e5d0502f985",
   "metadata": {},
   "source": [
    "## Classification\n",
    "Mistral models can easily categorize text into distinct classes. In this example prompt, we can define a list of predefined categories and ask Mistral models to classify user inquiry. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71e9d1c-ca45-4d19-882c-07e077ea19ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_message(inquiry):\n",
    "    user_message = (\n",
    "        f\"\"\"\n",
    "        You are a bank customer service bot. Your task is to assess customer intent \n",
    "        and categorize customer inquiry after <<<>>> into one of the following predefined categories:\n",
    "        \n",
    "        card arrival\n",
    "        change pin\n",
    "        exchange rate\n",
    "        country support \n",
    "        cancel transfer\n",
    "        charge dispute\n",
    "        \n",
    "        If the text doesn't fit into any of the above categories, classify it as:\n",
    "        customer service\n",
    "        \n",
    "        You will only respond with the predefined category. Do not include the word \"Category\". Do not provide explanations or notes. \n",
    "        \n",
    "        ####\n",
    "        Here are some examples:\n",
    "        \n",
    "        Inquiry: How do I know if I will get my card, or if it is lost? I am concerned about the delivery process and would like to ensure that I will receive my card as expected. Could you please provide information about the tracking process for my card, or confirm if there are any indicators to identify if the card has been lost during delivery?\n",
    "        Category: card arrival\n",
    "        Inquiry: I am planning an international trip to Paris and would like to inquire about the current exchange rates for Euros as well as any associated fees for foreign transactions.\n",
    "        Category: exchange rate \n",
    "        Inquiry: What countries are getting support? I will be traveling and living abroad for an extended period of time, specifically in France and Germany, and would appreciate any information regarding compatibility and functionality in these regions.\n",
    "        Category: country support\n",
    "        Inquiry: Can I get help starting my computer? I am having difficulty starting my computer, and would appreciate your expertise in helping me troubleshoot the issue. \n",
    "        Category: customer service\n",
    "        ###\n",
    "    \n",
    "        <<<\n",
    "        Inquiry: {inquiry}\n",
    "        >>>\n",
    "        \"\"\"\n",
    "    )\n",
    "    return user_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863460a-7670-4b6f-b511-cf1df7693cfa",
   "metadata": {},
   "source": [
    "### Strategies we used: \n",
    "\n",
    "- **Few shot learning**: Few-shot learning or in-context learning is when we give a few examples in the prompts, and the LLM can generate corresponding output based on the example demonstrations. Few-shot learning can often improve model performance especially when the task is difficult or when we want the model to respond in a specific manner. \n",
    "- **Delimiter**: Delimiters like ### <<< >>> specify the boundary between different sections of the text. In our example, we used ### to indicate examples and <<<>>> to indicate customer inquiry. \n",
    "- **Role playing**: Providing LLM a role (e.g., \"You are a bank customer service bot.\") adds personal context to the model and often leads to better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8a83cc-31e6-4d4b-b252-93d9133ecbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country support\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(user_message(\n",
    "    \"I am inquiring about the availability of your cards in the EU, as I am a resident of France and am interested in using your cards. \"\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6eca06b-7b1d-4663-9a68-2a23116f8c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer service\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(user_message(\"What's the weather today?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f351f00-683a-4244-974f-5c719812141f",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "\n",
    "Summarization is a common task for LLMs due to their natural language understanding and generation capabilities. Here is an example prompt we can use to generate interesting questions about an essay and summarize the essay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bbab492-1d18-4832-86a9-2fba645e0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')\n",
    "# need to limit the size to meet the token limit of the GitHub Models free service\n",
    "essay = response.text[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaede63d-7392-4f1c-8a87-507ee31fe246",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "You are a commentator. Your task is to write a report on an essay. \n",
    "When presented with the essay, come up with interesting questions to ask, and answer each question. \n",
    "Afterward, combine all the information and write a report in the markdown format. \n",
    "\n",
    "# Essay: \n",
    "{essay}\n",
    "\n",
    "# Instructions: \n",
    "## Summarize:\n",
    "In clear and concise language, summarize the key points and themes presented in the essay.\n",
    "\n",
    "## Interesting Questions: \n",
    "Generate three distinct and thought-provoking questions that can be asked about the content of the essay. For each question:\n",
    "- After \"Q: \", describe the problem \n",
    "- After \"A: \", provide a detailed explanation of the problem addressed in the question.\n",
    "- Enclose the ultimate answer in <>.\n",
    "\n",
    "## Write a report \n",
    "Using the essay summary and the answers to the interesting questions, create a comprehensive report in Markdown format. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5505b0a5-411b-4804-aaef-ccecfa3d07be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report\n",
      "\n",
      "## Summary\n",
      "\n",
      "The essay is a personal account of the author's journey through various interests and careers, from writing and programming to philosophy, artificial intelligence, and finally, painting. The author begins by discussing their early attempts at writing short stories and programming on an IBM 1401 during their school days. They express their initial fascination with philosophy, which they later found disappointing in college. They then delve into their exploration of AI, which they eventually realized was not as promising as they had initially thought. This realization led them to focus on Lisp, a programming language they found interesting in its own right. Despite their love for building things, the author felt dissatisfied with the transient nature of systems work, leading them to pursue painting as a means of creating something lasting.\n",
      "\n",
      "## Interesting Questions\n",
      "\n",
      "**Q1: How did the author's perception of philosophy change from high school to college?**\n",
      "\n",
      "*A1:* In high school, the author perceived philosophy as the study of ultimate truths, a field that seemed more powerful than others. However, upon entering college, they discovered that philosophy had been relegated to dealing with edge cases that other fields ignored. This realization led to the author's disenchantment with the subject, as they found the philosophy courses they took to be boring.\n",
      "\n",
      "**Q2: What led to the author's disillusionment with AI?**\n",
      "\n",
      "*A2:* The author initially saw great potential in AI, inspired by a novel and a documentary featuring intelligent computers. However, during their first year of grad school, they realized that AI, as practiced at the time, was unable to bridge the gap between processing a subset of natural language and truly understanding it. This realization led the author to conclude that the prevailing approach to AI was a hoax, prompting them to shift their focus to Lisp.\n",
      "\n",
      "**Q3: Why did the author turn to painting after working in the field of computer science?**\n",
      "\n",
      "*A3:* The author was drawn to painting as a means of creating something lasting, as they found the transient nature of systems work in computer science unsatisfying. They were also attracted to the independence that being an artist could offer, as it would free them from having a boss or the need to secure research funding.\n",
      "\n",
      "## Report\n",
      "\n",
      "The author's journey through various academic and professional fields is marked by a series of shifts in interest and disillusionments. Beginning with an early fascination with writing and programming, the author's attention turned to philosophy, which they perceived as a study of ultimate truths. However, their college experience revealed that philosophy had been relegated to dealing with edge cases ignored by other fields, leading to the author's disenchantment with the subject.\n",
      "\n",
      "The author then turned their attention to artificial intelligence, inspired by a novel and a documentary featuring intelligent computers. However, they soon realized that the prevalent approach to AI was unable to bridge the gap between processing a subset of natural language and truly understanding it. This realization led the author to conclude that AI, as practiced at the time, was a hoax, prompting them to shift their focus to Lisp, a programming language they found interesting in its own right.\n",
      "\n",
      "Despite their love for building things, the author felt dissatisfied with the transient nature of systems work in computer science. This dissatisfaction led them to pursue painting as a means of creating something lasting. They were also attracted to the independence that being an artist could offer, as it would free them from having a boss or the need to secure research funding. The author's journey thus reflects a search for meaning, permanence, and independence in their work.\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c1535e-1156-46bc-8e28-5f71846dbe65",
   "metadata": {},
   "source": [
    "## Strategies we used: \n",
    "\n",
    "- **Step-by-step instructions**: This strategy is inspired by the chain-of-thought prompting that enables LLMs to use a series of intermediate reasoning steps to tackle complex tasks. It's often easier to solve complex problems when we decompose them into simpler and small steps and it's easier for us to debug and inspect the model behavior.  In our example, we break down the task into three steps: summarize, generate interesting questions, and write a report. This helps the language to think in each step and generate a more comprehensive final report. \n",
    "- **Example generation**: We can ask LLMs to automatically guide the reasoning and understanding process by generating examples with the explanations and steps. In this example, we ask the LLM to generate three questions and provide detailed explanations for each question. \n",
    "- **Output formatting**: We can ask LLMs to output in a certain format by directly asking \"write a report in the Markdown format\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a9cb95-bb08-4929-b16c-eb19877f3c01",
   "metadata": {},
   "source": [
    "## Personlization\n",
    "\n",
    "LLMs excel at personalization tasks as they can deliver content that aligns closely with individual users. In this example, we create personalized email responses to address customer questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf048b4-3e33-4753-af97-25b73c51ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "email = \"\"\"\n",
    "Dear mortgage lender, \n",
    "\n",
    "What's your 30-year fixed-rate APR, how is it compared to the 15-year fixed rate?\n",
    "\n",
    "Regards,\n",
    "Anna\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36de7c1e-60c2-4f35-a51a-115b12d65bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "\n",
    "You are a mortgage lender customer service bot, and your task is to create personalized email responses to address customer questions.\n",
    "Answer the customer's inquiry using the provided facts below. Ensure that your response is clear, concise, and \n",
    "directly addresses the customer's question. Address the customer in a friendly and professional manner. Sign the email with \n",
    "\"Lender Customer Support.\"   \n",
    "\n",
    "\n",
    "      \n",
    "# Facts\n",
    "30-year fixed-rate: interest rate 6.403%, APR 6.484%\n",
    "20-year fixed-rate: interest rate 6.329%, APR 6.429%\n",
    "15-year fixed-rate: interest rate 5.705%, APR 5.848%\n",
    "10-year fixed-rate: interest rate 5.500%, APR 5.720%\n",
    "7-year ARM: interest rate 7.011%, APR 7.660%\n",
    "5-year ARM: interest rate 6.880%, APR 7.754%\n",
    "3-year ARM: interest rate 6.125%, APR 7.204%\n",
    "30-year fixed-rate FHA: interest rate 5.527%, APR 6.316%\n",
    "30-year fixed-rate VA: interest rate 5.684%, APR 6.062%\n",
    "\n",
    "# Email\n",
    "{email}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaf50774-0f91-4e0e-86c9-1525f6045ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Information on Our 30-Year and 15-Year Fixed-Rate Mortgages\n",
      "\n",
      "Dear Anna,\n",
      "\n",
      "Thank you for reaching out to us with your inquiry about our fixed-rate mortgage options. We are committed to providing you with the information you need to make an informed decision.\n",
      "\n",
      "Our current 30-year fixed-rate mortgage has an Annual Percentage Rate (APR) of 6.484%. In comparison, the 15-year fixed-rate mortgage has a lower APR of 5.848%. The difference in APR between the two options reflects the longer duration and associated risks of the 30-year fixed-rate mortgage. However, choosing a shorter term can result in higher monthly payments, even though you will pay less in interest over the life of the loan.\n",
      "\n",
      "We encourage you to consider your financial goals and budget when selecting the mortgage term that best suits your needs. Our team is available to provide further assistance and discuss your options in more detail.\n",
      "\n",
      "Please feel free to reply to this email or call us directly if you have any additional questions or would like to move forward with the application process.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Lender Customer Support\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af53ac9-8ef1-4840-8b23-779e1d59204d",
   "metadata": {},
   "source": [
    "### Strategies we used: \n",
    "- Providing facts: Incorporating facts into prompts can be useful for developing customer support bots. It’s important to use clear and concise language when presenting these facts. This can help the LLM to provide accurate and quick responses to customer queries.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "There are many ways to evaluate LLM outputs. Here are three approaches for your reference: include a confidence score, introduce an evaluation step, or employ another LLM for evaluation.\n",
    "\n",
    "\n",
    "\n",
    "## Include a confidence score\n",
    "We can include a confidence score along with the generated output in the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bf511a7-1828-465c-92ed-a701e21b23da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mistral(user_message, model=model_name):\n",
    "    client = MistralClient(api_key=github_token, endpoint=endpoint)\n",
    "    messages = [\n",
    "        ChatMessage(role=\"user\", content=user_message)\n",
    "    ]\n",
    "    chat_response = client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1\n",
    "    )\n",
    "    return (chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8087cae2-9b3b-407c-b808-31fd765fd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "You are a summarization system that can provide summaries with associated confidence scores.\n",
    "In clear and concise language, provide three short summaries of the following essay, along with their confidence scores. \n",
    "You will only respond with a JSON object with the key Summary and Confidence. Do not provide explanations. \n",
    "\n",
    "# Essay: \n",
    "{essay}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c7bfcee-ff8b-4c14-b9e8-7e00d3d389ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"Summary 1\": \"The author recounts their early experiences with writing and programming, starting with writing short stories and moving on to programming on an IBM 1401 in 9th grade. They found the 1401 limited and were excited by the advent of microcomputers, eventually convincing their father to buy a TRS-80.\",\n",
      "\"Confidence 1\": \"0.95\"\n",
      "},\n",
      "{\n",
      "\"Summary 2\": \"The author attended college planning to study philosophy, but found it boring and switched to AI after being inspired by a novel and a PBS documentary. They started learning Lisp and reversed-engineered SHRDLU for their undergraduate thesis.\",\n",
      "\"Confidence 2\": \"0.9\"\n",
      "},\n",
      "{\n",
      "\"Summary 3\": \"During grad school, the author realized that the approach to AI at the time was a 'hoax' and decided to focus on Lisp instead, even writing a book about Lisp hacking. They wanted to build things that would last and considered art as a possible career path.\",\n",
      "\"Confidence 3\": \"0.85\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(run_mistral(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649ad18-10c0-4fb0-92cc-052e2ab75342",
   "metadata": {},
   "source": [
    "### Strategies we used: \n",
    "- JSON output: For facilitating downstream tasks, JSON format output is frequently preferred. We can specify in the prompt that “You will only respond with a JSON object with the key Summary and Confidence.” Specifying these keys within the JSON object is beneficial for clarity and consistency.\n",
    "- Higher Temperature: In this example, we increase the temperature score to encourage the model to be more creative and output three generated summaries that are different from each other.  \n",
    "\n",
    "\n",
    "## Introduce an evaluation step \n",
    "We can also add a second step in the prompt for evaluation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6300a485-43be-4bc4-9088-9e6c2a365b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "\n",
      "Summary 1:\n",
      "The essay recounts the author's journey from writing and programming in their youth to studying philosophy and AI in college, and their eventual disillusionment with AI. The author then turns to Lisp and writes a book about it, while also nurturing an interest in painting.\n",
      "\n",
      "Summary 2:\n",
      "The author narrates their early experiences with writing, programming, and studying philosophy and AI in college. They realize the limitations of AI and shift their focus to Lisp, writing a book called \"On Lisp\". The author also develops a keen interest in painting and decides to pursue it professionally.\n",
      "\n",
      "Summary 3:\n",
      "The author shares their journey from being a young writer and programmer to studying philosophy and AI in college. They become disenchanted with AI and turn to Lisp, writing a book on the subject. Meanwhile, they discover a passion for painting and decide to make it their career.\n",
      "\n",
      "Step 2:\n",
      "\n",
      "The best summary among the three is Summary 2. This summary effectively captures the sequence of events, the author's shifting interests, and the key decisions they made. It clearly mentions the author's early experiences, their realization about AI, their focus on Lisp, and their decision to pursue painting. The clarity, completeness, and relevance to the essay's content make this summary the most effective.\n"
     ]
    }
   ],
   "source": [
    "message = f\"\"\"\n",
    "You are given an essay text and need to provide summaries and evaluate them.\n",
    "\n",
    "# Essay: \n",
    "{essay}\n",
    "\n",
    "Step 1: In this step, provide three short summaries of the given essay. Each summary should be clear, concise, and capture the key points of the speech. Aim for around 2-3 sentences for each summary.\n",
    "Step 2: Evaluate the three summaries from Step 1 and rate which one you believe is the best. Explain your choice by pointing out specific reasons such as clarity, completeness, and relevance to the speech content.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "print(run_mistral(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba076c25-9536-4ba1-a730-0a3ce6fe24f0",
   "metadata": {},
   "source": [
    "## Employ another LLM for evaluation\n",
    "In production systems, it is common to employ another LLM for evaluation so that the evaluation step can be separate from the generation step.  \n",
    "\n",
    "- Step 1: use the first LLM to generate three summaries \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5265085-77fd-433a-8d9d-d89ffac76543",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Provide three short summaries of the given essay. Each summary should be clear, concise, and capture the key points of the essay.\n",
    "Aim for around 2-3 sentences for each summary.\n",
    "\n",
    "# essay: \n",
    "{essay}\n",
    "\n",
    "\"\"\"\n",
    "summaries = run_mistral(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4372a9fe-d596-4f36-86c6-3e3c01c74c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The author recounts their early experiences with writing and programming before college. They wrote short stories, which they now realize were not very good, and tried programming on an IBM 1401, but found it limited. It wasn't until microcomputers became available that they really started programming, writing games and other simple programs.\n",
      "\n",
      "2. The author initially planned to study philosophy in college, but found it boring and switched to AI after being inspired by a novel and a documentary. They started learning Lisp and working on AI projects, but eventually realized that the type of AI they were working on was not going to lead to true understanding of natural language.\n",
      "\n",
      "3. After becoming disillusioned with AI, the author decided to focus on Lisp and write a book about it. They also became interested in painting and took art classes at Harvard. They eventually graduated from a PhD program in computer science and went to art school in Florence, where they painted still lives and learned about the low end eating the high end in technology.\n"
     ]
    }
   ],
   "source": [
    "print(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd5f25-ab9a-4547-83ab-48638517b7ef",
   "metadata": {},
   "source": [
    "- Step 2: use another LLM to rate the generated summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b755adc8-c277-4421-8925-3d70e5ee39e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 3 is the best summary of the essay. It provides a clear and complete overview of the author's experiences with writing, programming, and AI, as well as their eventual shift towards focusing on Lisp and painting. It highlights the key turning points in the author's career, such as their realization that the type of AI they were working on was not going to be successful, and their decision to write a book about Lisp and pursue painting. The summary also includes specific details about the author's experiences, such as their interest in still life painting and their observations about the low end eating the high end in technology. Overall, Summary 3 effectively captures the main points of the essay and provides a well-rounded and relevant summary of the author's experiences.\n"
     ]
    }
   ],
   "source": [
    "message = f\"\"\"\n",
    "You are given an essay and three summaries of the essay. Evaluate the three summaries and rate which one you believe is the best. \n",
    "Explain your choice by pointing out specific reasons such as clarity, completeness, and relevance to the essay content.\n",
    "\n",
    "# Essay: \n",
    "{essay}\n",
    "\n",
    "# Summaries \n",
    "{summaries}\n",
    "\n",
    "\"\"\"\n",
    "print(run_mistral(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e96d61a-9dbc-4f8c-9d80-d2260c5f3e61",
   "metadata": {},
   "source": [
    "### Strategies we used: \n",
    "- **LLM chaining**: In this example, we chain two LLMs in a sequence, where the output from the first LLM serves as the input for the second LLM. The method of chaining LLMs can be adapted to suit your specific use cases. For instance, you might choose to employ three LLMs in a chain, where the output of two LLMs is funneled into the third LLM. While LLM chaining offers flexibility, it's important to consider that it may result in additional API calls and potentially increased costs.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
